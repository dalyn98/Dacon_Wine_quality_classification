{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "269e1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import patches\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score,KFold,cross_validate\n",
    "from sklearn.metrics  import classification_report ,make_scorer,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a02f39",
   "metadata": {},
   "source": [
    "Non HyperParamter 에서 모델들의 성능 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39414429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model():\n",
    "    def __init__(self,data,zscore=False,smote = False,adasyn=False,drop=None,std=False):\n",
    "        \n",
    "        self.df = data\n",
    "        if drop is not None:\n",
    "            data = data.drop(drop,axis=1)\n",
    "        self.word_num()\n",
    "        self.data = data\n",
    "        if zscore==True:\n",
    "            data = self.zscore_df(data)\n",
    "        self.X = data.drop(['id', 'quality'],axis=1)\n",
    "        self.y = data['quality']\n",
    "        if smote==True:\n",
    "            self.X, self.y = SMOTE(random_state=0).fit_resample(self.X,self.y)\n",
    "        elif adasyn==True:\n",
    "            self.X, self.y = ADASYN(random_state=0).fit_resample(self.X,self.y)\n",
    "        elif std == True:\n",
    "            self.X = StandardScaler().fit_transform(self.X)\n",
    "        self.X_train,self.X_test,self.y_train,self.y_test = train_test_split(self.X,self.y,\n",
    "                                                                            test_size=0.4,\n",
    "                                                                            random_state=42)\n",
    "        \n",
    "    def data_return(self):\n",
    "        return self.X_train,self.X_test,self.y_train,self.y_test\n",
    "    def Logistic(self,linear = 'liblinear'):\n",
    "        model = LogisticRegression(solver = linear)\n",
    "        model.fit(self.X_train,self.y_train)\n",
    "        prediction = model.predict(self.X_test)\n",
    "        score = self.evaluate_model(self.X_test,self.y_test,model)\n",
    "        print(f\"모델의 정확도는 {np.mean(score)}% 입니다\")\n",
    "        self.__init__(self.data)\n",
    "        \n",
    "        return model\n",
    "    def grid_model(self,model_type,param=None):\n",
    "        model = GridSearchCV(model_type,param,scoring = 'accuracy',cv=3)\n",
    "        model = model_type\n",
    "        model.fit(self.X_train,self.y_train)\n",
    "        prediction = model.predict(self.X_test)\n",
    "        score = self.evaluate_model(self.X_test,self.y_test,model)\n",
    "        self.__init__(self.data)\n",
    "        print(f\"모델의 정확도는 {np.mean(score)}% 입니다\")\n",
    "        return model\n",
    "    def count_plot(self): \n",
    "        counter = Counter(self.y)\n",
    "        for k,v in counter.items():\n",
    "            print('Class=%d, n=%d (%.3f%%)' % (k, v, v / len(y) * 100))\n",
    "        pyplot.bar(counter.keys(), counter.values())\n",
    "        pyplot.show()\n",
    "    def word_num(self):\n",
    "        word_to_num = {'white' : 0, 'red' : 1}\n",
    "        self.df['type'] = self.df['type'].replace(word_to_num)\n",
    "    def evaluate_model(self,X, y, model):\n",
    "        cv = KFold(n_splits=10, shuffle=True)\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        return scores\n",
    "    def zscore_df(self,df):\n",
    "        \n",
    "        data = df.iloc[:,1:-1].apply(stats.zscore)\n",
    "        \n",
    "        data['quality'] = df['quality']\n",
    "        data['id'] = df['id']\n",
    "        #data['type'] = df['type']\n",
    "        \n",
    "        for i in data.iloc[:,:-2].columns:\n",
    "            data = data.loc[(data[i] > -2) & (data[i] < 2)]\n",
    "       \n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032306aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, model):\n",
    "        cv = KFold(n_splits=10, shuffle=True)\n",
    "        scores = cross_val_score(model, X, y, cv=cv, scoring=make_scorer(accuracy_score),n_jobs=-1)\n",
    "        return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X, y, model):\n",
    "        cv = KFold(n_splits=10, shuffle=True)\n",
    "        scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "        return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da59316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../train/train.csv'\n",
    "test_path = '../test/test.csv'\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "98248324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total sulfur dioxide 와 free sulfur dioxide의 상관계수가 높기때문에 삭제\n",
    "# -> 별다른 결과가 나타나지 않음\n",
    "# 와인의 품질을 7이상은 high, 미만은 low로 지정하여 feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "fcee1603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>type</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "      <td>3231.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1616.000000</td>\n",
       "      <td>7.205772</td>\n",
       "      <td>0.336072</td>\n",
       "      <td>0.319496</td>\n",
       "      <td>5.454813</td>\n",
       "      <td>0.055890</td>\n",
       "      <td>30.583720</td>\n",
       "      <td>116.256577</td>\n",
       "      <td>0.994667</td>\n",
       "      <td>3.214166</td>\n",
       "      <td>0.531455</td>\n",
       "      <td>10.497108</td>\n",
       "      <td>0.240792</td>\n",
       "      <td>5.829155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>932.853686</td>\n",
       "      <td>1.295494</td>\n",
       "      <td>0.160285</td>\n",
       "      <td>0.145854</td>\n",
       "      <td>4.816098</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>17.387143</td>\n",
       "      <td>55.759070</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>0.161873</td>\n",
       "      <td>0.149686</td>\n",
       "      <td>1.193813</td>\n",
       "      <td>0.427631</td>\n",
       "      <td>0.850003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>808.500000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>0.227500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.992205</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1616.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>0.994840</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2423.500000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>0.996900</td>\n",
       "      <td>3.320000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3231.000000</td>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.040000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>65.800000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>344.000000</td>\n",
       "      <td>1.038980</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>1.980000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id  fixed acidity  volatile acidity  citric acid  \\\n",
       "count  3231.000000    3231.000000       3231.000000  3231.000000   \n",
       "mean   1616.000000       7.205772          0.336072     0.319496   \n",
       "std     932.853686       1.295494          0.160285     0.145854   \n",
       "min       1.000000       3.800000          0.080000     0.000000   \n",
       "25%     808.500000       6.400000          0.227500     0.250000   \n",
       "50%    1616.000000       7.000000          0.290000     0.310000   \n",
       "75%    2423.500000       7.700000          0.400000     0.390000   \n",
       "max    3231.000000      15.900000          1.040000     1.660000   \n",
       "\n",
       "       residual sugar    chlorides  free sulfur dioxide  total sulfur dioxide  \\\n",
       "count     3231.000000  3231.000000          3231.000000           3231.000000   \n",
       "mean         5.454813     0.055890            30.583720            116.256577   \n",
       "std          4.816098     0.035722            17.387143             55.759070   \n",
       "min          0.600000     0.012000             1.000000              6.000000   \n",
       "25%          1.800000     0.038000            17.000000             78.000000   \n",
       "50%          3.100000     0.047000            29.000000            119.000000   \n",
       "75%          8.100000     0.064000            42.000000            156.000000   \n",
       "max         65.800000     0.611000           131.000000            344.000000   \n",
       "\n",
       "           density           pH    sulphates      alcohol         type  \\\n",
       "count  3231.000000  3231.000000  3231.000000  3231.000000  3231.000000   \n",
       "mean      0.994667     3.214166     0.531455    10.497108     0.240792   \n",
       "std       0.003054     0.161873     0.149686     1.193813     0.427631   \n",
       "min       0.987110     2.720000     0.220000     8.400000     0.000000   \n",
       "25%       0.992205     3.100000     0.430000     9.500000     0.000000   \n",
       "50%       0.994840     3.200000     0.510000    10.300000     0.000000   \n",
       "75%       0.996900     3.320000     0.600000    11.300000     0.000000   \n",
       "max       1.038980     4.010000     1.980000    14.900000     1.000000   \n",
       "\n",
       "           quality  \n",
       "count  3231.000000  \n",
       "mean      5.829155  \n",
       "std       0.850003  \n",
       "min       4.000000  \n",
       "25%       5.000000  \n",
       "50%       6.000000  \n",
       "75%       6.000000  \n",
       "max       8.000000  "
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81ace783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 정확도는 0.4247524752475248% 입니다\n"
     ]
    }
   ],
   "source": [
    "train_model = Model(train_df,zscore=True,smote=True,drop='total sulfur dioxide').Logistic()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f19d088",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 정확도는 0.6168316831683167% 입니다\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "regressor = DecisionTreeClassifier()\n",
    "\n",
    "param_grid ={'max_depth':(1,2,3,4,5,6,7,8,9,10)}\n",
    "\n",
    "model = Model(train_df,zscore=True,smote=True,drop='total sulfur dioxide')\n",
    "train_model = model.grid_model(regressor,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476041b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a879ea058d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                  'random_state':[2]}\n\u001b[0;32m     14\u001b[0m \u001b[0mmmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'total sulfur dioxide'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgbr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgbr = XGBClassifier()\n",
    "param_grid = param_grid={'booster' :['gbtree'],\n",
    "                 'silent':[True],\n",
    "                 'max_depth':[5,6,8],\n",
    "                 'min_child_weight':[1,3,5],\n",
    "                 'gamma':[0,1,2,3],\n",
    "                 'nthread':[4],\n",
    "                 'colsample_bytree':[0.5,0.8],\n",
    "                 'colsample_bylevel':[0.9],\n",
    "                 'n_estimators':[50],\n",
    "                 'objective':['binary:logistic'],\n",
    "                 'random_state':[2]}\n",
    "mmodel = Model(train_df,zscore=True,drop='total sulfur dioxide')\n",
    "train_model = model.grid_model(xgbr,param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "297e7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('../test/test.csv')\n",
    "word_to_num = {\"white\":0, \"red\":1}\n",
    "\n",
    "test['type'] = test['type'].replace(word_to_num)\n",
    "test =  test.apply(stats.zscore)\n",
    "prediction = train_model.predict(test.drop('id', axis=1))\n",
    "\n",
    "\n",
    "submission = pd.read_csv('../sample_submission.csv')\n",
    "submission['quality'] = prediction\n",
    "submission.to_csv(\"../submission/xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dccde392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1756\n",
       "1     465\n",
       "Name: goodquality, dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['goodquality'] = [1 if x >= 7 else 0 for x in df['quality']]\n",
    "# df['goodquality'].value_counts()                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "121ede7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>type</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.040</td>\n",
       "      <td>15.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.99120</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.61</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.021</td>\n",
       "      <td>38.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99026</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.48</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.021</td>\n",
       "      <td>26.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.98860</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.30</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.50</td>\n",
       "      <td>10.70</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.60</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.43</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.063</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.99550</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.81</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>3227</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.043</td>\n",
       "      <td>42.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.99455</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.47</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>3228</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>17.05</td>\n",
       "      <td>0.047</td>\n",
       "      <td>53.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.99724</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.35</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3228</th>\n",
       "      <td>3229</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.042</td>\n",
       "      <td>8.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.99290</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3229</th>\n",
       "      <td>3230</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.041</td>\n",
       "      <td>29.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.99520</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3230</th>\n",
       "      <td>3231</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.30</td>\n",
       "      <td>0.034</td>\n",
       "      <td>33.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.99530</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>9.3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3231 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "0        1            8.0              0.18         0.37            1.30   \n",
       "1        2            7.5              0.38         0.29            4.90   \n",
       "2        3            6.1              0.27         0.33            2.20   \n",
       "3        4            6.4              0.32         0.50           10.70   \n",
       "4        5            8.4              0.37         0.43            2.30   \n",
       "...    ...            ...               ...          ...             ...   \n",
       "3226  3227            6.4              0.16         0.31            5.30   \n",
       "3227  3228            6.4              0.18         0.28           17.05   \n",
       "3228  3229            6.0              0.28         0.25            1.80   \n",
       "3229  3230            7.7              0.28         0.29            6.90   \n",
       "3230  3231            7.1              0.15         0.34            5.30   \n",
       "\n",
       "      chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
       "0         0.040                 15.0                  96.0  0.99120  3.06   \n",
       "1         0.021                 38.0                 113.0  0.99026  3.08   \n",
       "2         0.021                 26.0                 117.0  0.98860  3.12   \n",
       "3         0.047                 57.0                 206.0  0.99680  3.08   \n",
       "4         0.063                 12.0                  19.0  0.99550  3.17   \n",
       "...         ...                  ...                   ...      ...   ...   \n",
       "3226      0.043                 42.0                 157.0  0.99455  3.35   \n",
       "3227      0.047                 53.0                 139.0  0.99724  3.25   \n",
       "3228      0.042                  8.0                 108.0  0.99290  3.08   \n",
       "3229      0.041                 29.0                 163.0  0.99520  3.44   \n",
       "3230      0.034                 33.0                 104.0  0.99530  3.37   \n",
       "\n",
       "      sulphates  alcohol  type  quality  \n",
       "0          0.61     12.1     0        6  \n",
       "1          0.48     13.0     0        7  \n",
       "2          0.30     12.5     0        6  \n",
       "3          0.60      9.4     0        5  \n",
       "4          0.81     11.2     1        7  \n",
       "...         ...      ...   ...      ...  \n",
       "3226       0.47     10.5     0        5  \n",
       "3227       0.35     10.5     0        6  \n",
       "3228       0.55      9.0     0        5  \n",
       "3229       0.60     10.5     0        6  \n",
       "3230       0.52      9.3     0        7  \n",
       "\n",
       "[3231 rows x 14 columns]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "a6fe52a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zscore\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.05      0.10        19\n",
      "           5       0.62      0.61      0.61       290\n",
      "           6       0.53      0.75      0.62       402\n",
      "           7       0.54      0.14      0.23       153\n",
      "           8       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.56       889\n",
      "   macro avg       0.54      0.31      0.31       889\n",
      "weighted avg       0.56      0.56      0.52       889\n",
      "\n",
      "0.5546475995914197\n",
      "zscore + drop type\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.05      0.10        19\n",
      "           5       0.63      0.61      0.62       290\n",
      "           6       0.54      0.75      0.63       402\n",
      "           7       0.51      0.14      0.22       153\n",
      "           8       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.56       889\n",
      "   macro avg       0.54      0.31      0.31       889\n",
      "weighted avg       0.56      0.56      0.53       889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5523748723186925\n",
      "zscore + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.53      0.68      0.60       386\n",
      "           5       0.47      0.50      0.48       404\n",
      "           6       0.28      0.15      0.20       393\n",
      "           7       0.47      0.33      0.39       403\n",
      "           8       0.46      0.65      0.54       400\n",
      "\n",
      "    accuracy                           0.46      1986\n",
      "   macro avg       0.44      0.46      0.44      1986\n",
      "weighted avg       0.44      0.46      0.44      1986\n",
      "\n",
      "0.4562306481904471\n",
      "std + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.52      0.72      0.60       544\n",
      "           5       0.42      0.43      0.42       589\n",
      "           6       0.31      0.15      0.21       583\n",
      "           7       0.40      0.32      0.36       561\n",
      "           8       0.44      0.56      0.49       559\n",
      "\n",
      "    accuracy                           0.43      2836\n",
      "   macro avg       0.42      0.44      0.42      2836\n",
      "weighted avg       0.41      0.43      0.41      2836\n",
      "\n",
      "0.4245122679540138\n",
      "std\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.02      0.04        49\n",
      "           5       0.57      0.62      0.59       418\n",
      "           6       0.55      0.72      0.63       586\n",
      "           7       0.56      0.19      0.29       206\n",
      "           8       0.00      0.00      0.00        34\n",
      "\n",
      "    accuracy                           0.56      1293\n",
      "   macro avg       0.54      0.31      0.31      1293\n",
      "weighted avg       0.56      0.56      0.52      1293\n",
      "\n",
      "0.5213357185450209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# X = df.drop(['quality','goodquality','id'], axis = 1) \n",
    "# y = df['goodquality']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True).data_return()\n",
    "print('zscore')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,smote=True).data_return()\n",
    "print('zscore + Smote')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type').data_return()\n",
    "print('zscore + drop type')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True).data_return()\n",
    "print('std')\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(classification_report(y_test, prediction))\n",
    "print(evaluate_model(X_test,y_test,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4644feb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zscore\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.09      0.16      0.12        19\n",
      "           5       0.56      0.55      0.55       290\n",
      "           6       0.57      0.56      0.56       402\n",
      "           7       0.41      0.39      0.40       153\n",
      "           8       0.28      0.28      0.28        25\n",
      "\n",
      "    accuracy                           0.51       889\n",
      "   macro avg       0.38      0.39      0.38       889\n",
      "weighted avg       0.52      0.51      0.51       889\n",
      "\n",
      "0.4780771195097038\n",
      "zscore + drop type\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.08      0.11      0.09        19\n",
      "           5       0.58      0.58      0.58       290\n",
      "           6       0.57      0.57      0.57       402\n",
      "           7       0.42      0.39      0.40       153\n",
      "           8       0.27      0.32      0.29        25\n",
      "\n",
      "    accuracy                           0.52       889\n",
      "   macro avg       0.38      0.39      0.39       889\n",
      "weighted avg       0.53      0.52      0.53       889\n",
      "\n",
      "0.4588866189989786\n",
      "zscore + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.83      0.89      0.86       386\n",
      "           5       0.65      0.66      0.66       404\n",
      "           6       0.53      0.49      0.51       393\n",
      "           7       0.68      0.71      0.70       403\n",
      "           8       0.89      0.86      0.88       400\n",
      "\n",
      "    accuracy                           0.72      1986\n",
      "   macro avg       0.72      0.72      0.72      1986\n",
      "weighted avg       0.72      0.72      0.72      1986\n",
      "\n",
      "0.6515684483021167\n",
      "std + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.78      0.84      0.81       544\n",
      "           5       0.60      0.58      0.59       589\n",
      "           6       0.48      0.42      0.44       583\n",
      "           7       0.65      0.68      0.66       561\n",
      "           8       0.82      0.86      0.84       559\n",
      "\n",
      "    accuracy                           0.67      2836\n",
      "   macro avg       0.66      0.67      0.67      2836\n",
      "weighted avg       0.66      0.67      0.67      2836\n",
      "\n",
      "0.6121286020006967\n",
      "std\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.16      0.10      0.12        49\n",
      "           5       0.58      0.57      0.58       418\n",
      "           6       0.58      0.56      0.57       586\n",
      "           7       0.43      0.52      0.47       206\n",
      "           8       0.29      0.35      0.32        34\n",
      "\n",
      "    accuracy                           0.53      1293\n",
      "   macro avg       0.41      0.42      0.41      1293\n",
      "weighted avg       0.53      0.53      0.53      1293\n",
      "\n",
      "0.4616756112104949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True).data_return()\n",
    "print('zscore')\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type').data_return()\n",
    "print('zscore + drop type')\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True).data_return()\n",
    "print('std')\n",
    "model = DecisionTreeClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "627dd936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "6f030d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zscore\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.11      0.19        19\n",
      "           5       0.66      0.62      0.64       290\n",
      "           6       0.57      0.73      0.64       402\n",
      "           7       0.56      0.33      0.41       153\n",
      "           8       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.60       889\n",
      "   macro avg       0.71      0.41      0.46       889\n",
      "weighted avg       0.61      0.60      0.59       889\n",
      "\n",
      "0.6029622063329929\n",
      "zscore + Smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.90      0.99      0.94       386\n",
      "           5       0.74      0.77      0.75       404\n",
      "           6       0.64      0.50      0.56       393\n",
      "           7       0.80      0.85      0.82       403\n",
      "           8       0.93      0.97      0.95       400\n",
      "\n",
      "    accuracy                           0.81      1986\n",
      "   macro avg       0.81      0.81      0.81      1986\n",
      "weighted avg       0.80      0.81      0.81      1986\n",
      "\n",
      "0.7653240952235927\n",
      "zscore + adasyn\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.91      0.97      0.94       398\n",
      "           5       0.69      0.80      0.74       392\n",
      "           6       0.70      0.43      0.53       396\n",
      "           7       0.80      0.89      0.84       455\n",
      "           8       0.92      0.95      0.94       399\n",
      "\n",
      "    accuracy                           0.81      2040\n",
      "   macro avg       0.80      0.81      0.80      2040\n",
      "weighted avg       0.80      0.81      0.80      2040\n",
      "\n",
      "0.75\n",
      "zscore + drop type\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.11      0.19        19\n",
      "           5       0.64      0.62      0.63       290\n",
      "           6       0.57      0.74      0.65       402\n",
      "           7       0.60      0.32      0.42       153\n",
      "           8       0.88      0.28      0.42        25\n",
      "\n",
      "    accuracy                           0.60       889\n",
      "   macro avg       0.74      0.41      0.46       889\n",
      "weighted avg       0.62      0.60      0.59       889\n",
      "\n",
      "0.5793411644535239\n",
      "zscore + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.90      0.98      0.94       386\n",
      "           5       0.74      0.76      0.75       404\n",
      "           6       0.66      0.51      0.57       393\n",
      "           7       0.79      0.87      0.83       403\n",
      "           8       0.94      0.96      0.95       400\n",
      "\n",
      "    accuracy                           0.82      1986\n",
      "   macro avg       0.81      0.82      0.81      1986\n",
      "weighted avg       0.81      0.82      0.81      1986\n",
      "\n",
      "0.7668874676412365\n",
      "std + drop type + smote\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.88      0.96      0.92       544\n",
      "           5       0.70      0.71      0.70       589\n",
      "           6       0.64      0.48      0.55       583\n",
      "           7       0.75      0.82      0.78       561\n",
      "           8       0.90      0.96      0.93       559\n",
      "\n",
      "    accuracy                           0.78      2836\n",
      "   macro avg       0.77      0.78      0.77      2836\n",
      "weighted avg       0.77      0.78      0.77      2836\n",
      "\n",
      "0.7288670183646045\n",
      "std\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.50      0.04      0.08        49\n",
      "           5       0.64      0.64      0.64       418\n",
      "           6       0.59      0.72      0.65       586\n",
      "           7       0.57      0.41      0.48       206\n",
      "           8       1.00      0.26      0.42        34\n",
      "\n",
      "    accuracy                           0.61      1293\n",
      "   macro avg       0.66      0.42      0.45      1293\n",
      "weighted avg       0.61      0.61      0.59      1293\n",
      "\n",
      "0.5916100178890877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True).data_return()\n",
    "print('zscore')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,smote=True).data_return()\n",
    "print('zscore + Smote')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,adasyn=True).data_return()\n",
    "print('zscore + adasyn')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type').data_return()\n",
    "print('zscore + drop type')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True).data_return()\n",
    "print('std')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "2035cbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(25).plot(kind='barh',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9323c83c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zscore\n",
      "[00:51:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.50      0.11      0.17        19\n",
      "           5       0.61      0.58      0.60       290\n",
      "           6       0.56      0.66      0.61       402\n",
      "           7       0.46      0.37      0.41       153\n",
      "           8       0.58      0.28      0.38        25\n",
      "\n",
      "    accuracy                           0.56       889\n",
      "   macro avg       0.54      0.40      0.43       889\n",
      "weighted avg       0.56      0.56      0.56       889\n",
      "\n",
      "0.55232379979571\n",
      "zscore + drop type\n",
      "[00:51:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.40      0.11      0.17        19\n",
      "           5       0.60      0.57      0.59       290\n",
      "           6       0.55      0.66      0.60       402\n",
      "           7       0.44      0.35      0.39       153\n",
      "           8       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.55       889\n",
      "   macro avg       0.55      0.39      0.43       889\n",
      "weighted avg       0.55      0.55      0.54       889\n",
      "\n",
      "0.5634959141981615\n",
      "zscore + drop type + smote\n",
      "[00:51:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.91      0.98      0.94       386\n",
      "           5       0.75      0.74      0.75       404\n",
      "           6       0.64      0.60      0.62       393\n",
      "           7       0.82      0.80      0.81       403\n",
      "           8       0.95      0.97      0.96       400\n",
      "\n",
      "    accuracy                           0.82      1986\n",
      "   macro avg       0.81      0.82      0.82      1986\n",
      "weighted avg       0.81      0.82      0.82      1986\n",
      "\n",
      "0.756316938226486\n",
      "std + drop type + smote\n",
      "[00:51:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.90      0.95      0.93       544\n",
      "           5       0.71      0.70      0.70       589\n",
      "           6       0.66      0.58      0.62       583\n",
      "           7       0.75      0.80      0.77       561\n",
      "           8       0.93      0.95      0.94       559\n",
      "\n",
      "    accuracy                           0.79      2836\n",
      "   macro avg       0.79      0.79      0.79      2836\n",
      "weighted avg       0.79      0.79      0.79      2836\n",
      "\n",
      "0.7313243418105808\n",
      "std\n",
      "[00:52:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.62      0.10      0.18        49\n",
      "           5       0.63      0.64      0.63       418\n",
      "           6       0.59      0.68      0.63       586\n",
      "           7       0.55      0.44      0.49       206\n",
      "           8       0.71      0.29      0.42        34\n",
      "\n",
      "    accuracy                           0.60      1293\n",
      "   macro avg       0.62      0.43      0.47      1293\n",
      "weighted avg       0.60      0.60      0.59      1293\n",
      "\n",
      "0.5530292188431722\n",
      "zscore + smote\n",
      "[00:52:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       0.91      0.97      0.94       386\n",
      "           5       0.74      0.75      0.74       404\n",
      "           6       0.63      0.59      0.61       393\n",
      "           7       0.80      0.80      0.80       403\n",
      "           8       0.96      0.96      0.96       400\n",
      "\n",
      "    accuracy                           0.81      1986\n",
      "   macro avg       0.81      0.82      0.81      1986\n",
      "weighted avg       0.81      0.81      0.81      1986\n",
      "\n",
      "0.7507816862088219\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True).data_return()\n",
    "print('zscore')\n",
    "\n",
    "\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type').data_return()\n",
    "print('zscore + drop type')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True).data_return()\n",
    "print('std')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,smote=True).data_return()\n",
    "\n",
    "print('zscore + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(evaluate_model(X_test,y_test,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f1de43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std + drop type + smote\n",
      "f1평균 :  0.7745523804476158 precision 평균 :  0.7718619721254619\n",
      "zscore + smote\n",
      "[00:55:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1평균 :  0.7907731341472723 precision 평균 :  0.788601198205504\n",
      "std + drop type + smote\n",
      "[00:55:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1평균 :  0.7907731341472723 precision 평균 :  0.788601198205504\n",
      "zscore + drop type + smote\n",
      "[00:55:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "f1평균 :  0.8154906901007528 precision 평균 :  0.813607138953216\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_mean = []\n",
    "accuracy_mean = []\n",
    "for i in ['4','5','6','7','8']:\n",
    "    f1_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['f1-score'])\n",
    "    accuracy_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['precision'])\n",
    "print('f1평균 : ',np.mean(f1_mean),'precision 평균 : ',np.mean(accuracy_mean))    \n",
    "\n",
    "print('zscore + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_mean = []\n",
    "accuracy_mean = []\n",
    "for i in ['4','5','6','7','8']:\n",
    "    f1_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['f1-score'])\n",
    "    accuracy_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['precision'])\n",
    "print('f1평균 : ',np.mean(f1_mean),'precision 평균 : ',np.mean(accuracy_mean)) \n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,std=True,smote=True,drop='type').data_return()\n",
    "print('std + drop type + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_mean = []\n",
    "accuracy_mean = []\n",
    "for i in ['4','5','6','7','8']:\n",
    "    f1_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['f1-score'])\n",
    "    accuracy_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['precision'])\n",
    "print('f1평균 : ',np.mean(f1_mean),'precision 평균 : ',np.mean(accuracy_mean)) \n",
    "\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "f1_mean = []\n",
    "accuracy_mean = []\n",
    "for i in ['4','5','6','7','8']:\n",
    "    f1_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['f1-score'])\n",
    "    accuracy_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['precision'])\n",
    "print('f1평균 : ',np.mean(f1_mean),'precision 평균 : ',np.mean(accuracy_mean))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "161b970e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9533169533169533,\n",
       " 'recall': 0.97,\n",
       " 'f1-score': 0.9615861214374226,\n",
       " 'support': 400}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred,output_dict = True)[i]['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "3c966bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_mean = []\n",
    "for i in ['4','5','6','7','8']:\n",
    "    f1_mean.append(classification_report(y_test, y_pred,output_dict = True)[i]['f1-score'])\n",
    "print('f1평균 : ',np.mean(f1_mean))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "16cee4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129709804220759"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('f1평균 : ',np.mean(f1_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d789acf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-004e829d950e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mzscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_return\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3193b511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e30ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('std + drop type + smote')\n",
    "k = range(1,50,2)\n",
    "testing_accuracy = []\n",
    "training_accuracy = []\n",
    "score = 0\n",
    "\n",
    "for i in tqdm(k):\n",
    "    knn = KNeighborsClassifier(n_neighbors = i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred_train = knn.predict(X_train)\n",
    "    training_accuracy.append(accuracy_score(y_train,y_pred_train))\n",
    "    \n",
    "    y_pred_test = knn.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test,y_pred_test)\n",
    "    testing_accuracy.append(acc_score)\n",
    "    \n",
    "    if score < acc_score:\n",
    "        score = acc_score\n",
    "        besk_k = i\n",
    "print('Best Accuracy Score ',score,'Best K-score',besk_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "6128b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "7679ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5830815709969789"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "aea9340d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zscore + drop type + smote\n",
      "[14:30:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = Model(train_df,zscore=True,drop='type',smote=True).data_return()\n",
    "print('zscore + drop type + smote')\n",
    "model = XGBClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "11f1a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('../test/test.csv')\n",
    "test.drop('type',axis=1,inplace=True)\n",
    "test =  test.apply(stats.zscore)\n",
    "prediction = model.predict(test.drop('id', axis=1))\n",
    "\n",
    "\n",
    "submission = pd.read_csv('../sample_submission.csv')\n",
    "submission['quality'] = prediction\n",
    "submission.to_csv(\"../submission/knn_zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "a5c802ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type에 관계한 부분은 xgboost에서 밖에 중요하다고 하지 않음\n",
    "# 상식적으로 생각해도 적/백 포도주 자체가 와인의 품질을 결정하진 않을것같음\n",
    "# 해당 데이터 삭제\n",
    "# -> 별다른 변화 없음\n",
    "# zscore + smote + drop type이 좀더 좋은 결과를 보여줌\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39322d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e31962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383313e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69d33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709cd500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee5f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6a99b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8315dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f6dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9490866e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d106337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
